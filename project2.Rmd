---
title: "Shares analysis"
author: "C-dog & Kray-Kray"
date: "October 30, 2021"
params:
  data: 1
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Introduction

We are using the shares of news stories datafile from the UCI Machine learning database. We are focusing on variables that describe the length of articles, sentiment of articles, subjectivity of articles, day of publishing, and content of the articles. Our goal is to create a model that can predict the number of shares based on either all variables or a subset of these variables. We will use linear, random forest, and boosted tree models. 

Libraries
```{r message=FALSE, warning=FALSE}
library("tidyverse")
library("GGally")
library("caret")
library("gbm")
library("doParallel")
set.seed(1234)
```
```{r}
page_name <- if_else(params$data == 1, "business", 
if_else(params$data == 2, "lifestyle",
if_else(params$data == 3, "entertainment",
if_else(params$data == 4, "social media",
if_else(params$data == 5, "technology", "world")))))
```

#Analysis of `r page_name` shares.

Read in and subset the data
```{r message=FALSE}
news <- read_csv("OnlineNewsPopularity.csv")

news2 <- news %>%  mutate(type = 
            if_else(data_channel_is_bus == 1, 1, 
            if_else( data_channel_is_lifestyle == 1, 2,
            if_else(data_channel_is_entertainment == 1 , 3, 
            if_else(data_channel_is_socmed == 1,  4 ,
            if_else(data_channel_is_tech == 1, 5, 6))))))
           

business <- news %>% filter(data_channel_is_bus == 1)
lifestyle <- news %>% filter(data_channel_is_lifestyle == 1)
entertainment <- news %>% filter(data_channel_is_entertainment == 1)
socmed <- news %>% filter(data_channel_is_socmed == 1)
tech <- news %>% filter(data_channel_is_tech == 1)
world <- news %>% filter(data_channel_is_world == 1)



# The sum of these rows does not equal the total number of rows in news.

# Check if there are unclassified observations
other <- news %>% filter((data_channel_is_lifestyle + data_channel_is_entertainment + data_channel_is_lifestyle + data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + data_channel_is_world) == 0)

# This accounts for all of the data. Life is good.
```

Set the Channel according to the parameter
```{r}
channel <-  news2 %>% filter(type == params$data)
```
  
  Now we have an excellent partitioning of the data.  By going to the website and reading the descriptions of the variables, we became interested to see if there was a time influence on the number of shares.  If an article was published on a weekday versus a weekend, would it get more shares?  We thought so, with an intuition that people read more articles at work, and thus would be more likely to share articles that publish during the work week.  Let's see if we were on to something, shall we...  
   


# Day of the week

We created a variable that indicated the day of the week an article was published so we could summarize the number of shares by day.    
```{r message=FALSE, warning=FALSE}

channel <- channel %>% mutate(day = 
            if_else(weekday_is_monday == 1, 'Monday', 
            if_else( weekday_is_tuesday == 1, 'Tuesday',
            if_else(weekday_is_wednesday == 1 , 'Wednesday', 
            if_else( weekday_is_thursday == 1 ,  'Thursday' ,
            if_else(weekday_is_friday == 1, 'Friday',
            if_else(weekday_is_saturday == 1, 'Saturday', 'Sunday')))))))

channel$day <- factor(channel$day, levels = c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))

# Creating 'workweek' variable for modeling (Mon - Thurs VS Fri - Sun):

channel <- channel %>% mutate(workweek = 
            if_else(weekday_is_monday == 1 | weekday_is_tuesday == 1 |
                      weekday_is_wednesday == 1 |  weekday_is_thursday == 1, 'yes', 'no'))

ggplot(data=channel, aes(x=day, y=shares)) +
  geom_col() +
  labs(x="Day of the week", y="Number of shares", title = "Day vs Shares")


```    
This graph indicates which day has the most shares. A higher bar indicates more shares of an article that was published that day.

```{r}
channel %>% group_by(day) %>% summarise(Total = sum(shares), mean = mean(shares), sd = sd(shares))
```

The summary statistics indicate the total number of shares from an article published that day along with the average and standard deviation. Days with the highest mean indicate more shares. A large standard deviation indicates high variability in the number of shares from an article published on that day.

#Multi media

```{R}
# Make a new variable combining images and videos
channel <- channel %>% mutate(multimedia = num_imgs + num_videos)
channel <- channel %>% mutate(multimedia_category = 
                                  if_else(multimedia >= 30, "high",
                                  if_else(multimedia >= 20, "medium", 
                                  if_else(multimedia >= 10, "low", "very low"))))
channel$multimedia_category <- factor(channel$multimedia_category, levels = c("very low", "low", "medium", "high"))

ggplot(data = channel, aes(x= multimedia, y=shares)) +
  geom_point() +
  labs(x="Number of images and videos", y= "Shares", title = "Multimedia vs shares")
```
This graph displays the number of images and videos against the number of shares. A positive trend would indicate that more images and videos are associatied with more shares. A negative trend would indicate that more images and videos are associated with fewer shares.

```{r}
ggplot(data=channel, aes(x=multimedia_category, y=shares)) +
  geom_col() +
  labs(x="Multimedia items", y = "Number of shares", title = "Media vs Shares")

```
This graph displays the number of shares based on the number of multi-media items in an article. A high bar indicates the category that was most prevalent. 

#Length of article

With today's shortened attention span, are shorter articles more likely to be shared?

```{r}
# First, what is the distribution of article length?
plot(channel$n_tokens_content)
results <- summary(news$n_tokens_content)

# Looks like the vast majority are 1000 words or less.  So we'll break them into quantiles: Blip, Short, Medium, Epic

channel <- channel %>% mutate(wordiness = if_else(n_tokens_content > results[[5]], 'Epic', if_else(n_tokens_content >results[[3]], 'Medium', if_else(n_tokens_content > results[[2]], 'Short', 'Blip'))))

channel$wordiness <- factor(channel$wordiness, levels = c("Blip", "Short", "Medium", "Epic"))

ggplot(data = channel, aes(x = wordiness, y=shares)) +
  geom_col() +
  labs(x="Wordiness", y= "Shares", title = "Wordiness vs shares")
```
The category with the highest bar indicates which grouping of lengths was most shared for this category of articles.

```{r}
# Summary of wordiness

channel %>% group_by(wordiness) %>% summarise(Total = sum(shares), mean = mean(shares), sd = sd(shares))


```
The summary statistics show the total number of shares based on article length.  It also reports the average and the standard deviation.  If the standard deviation is high, then it says there is much more variation in the number of shares. 

#Length of Title
How long is the title?
Is the title one word or a phrase?  Does length perhaps grab the reader's attention to read it and then to pass it on...let's look and see.

```{r}
# First, what is the distribution of title length?
plot(channel$n_tokens_title)
results <- summary(news$n_tokens_title)
results
# Good God!  A title 23 words long???  Isn't that a sentence and not a title??? 
# Well, let's grouping by quartiles:

channel <- channel %>% mutate(title_words = 
            if_else(n_tokens_title > results[[5]] , 'Long', 
                    if_else(n_tokens_title > results[[3]], 'Typical', 'Short')))


channel$title_words <- factor(channel$title_words, 
                               levels = c("Short", "Typical", "Long"))

ggplot(data = channel, aes(x = title_words, y = shares)) + 
  geom_col() +
  labs(x="Title Length", y= "Shares", title = "Title Length vs shares")


```
The category with the highest bar indicates the length that is most shared.  

Would a long article have a long title?  Is there a relationship between article length and title length?  If so, there might be an issue of correlation.

```{r message=FALSE, warning=FALSE}

table(channel$wordiness, channel$title_words)

```
The columns are title length, the rows are article length, each cell displays the count of articles in those categories. 

#Sentiment
Is it a positive article, or negative?

This code below compares the average number of shares based on positive and negative polarity of an article. If it is more positive than negative it is classified as positive, if not, negative. The summary statistics indicate the mean number of shares for articles classified as negative or positive and the standard deviation. A higher mean value would indicate more shares.

```{r}
channel <- channel %>% mutate(av_avg_neg_polarity = -1*avg_negative_polarity)
ggplot(data=channel, aes(x=av_avg_neg_polarity, y=avg_positive_polarity)) +
  geom_point() +
  stat_function(fun=function(x) x, size = 2, lty = 3 ,color = 'red') +
  labs(x= "Mean negative polarity", y= "Mean positive polarity", title = "Negative Polarity vs Positive Polarity")

```
This graph shows the relationship between the positive and negative polarity of an article. Values above the line y=x would indicate more positive and values below the line y=x indicate more negativity.


The graphs below the distribution of shares according to their positive and negative polarity rating. The peak of the distribution indicates what rating has the most shares.



```{r}
channel <- channel %>% mutate( sentiment = if_else(avg_positive_polarity > av_avg_neg_polarity, "positive", "negative"))

channel %>% group_by(sentiment) %>% summarise(mean = mean(shares), sd= sd(shares), sum = sum(shares), ratio = sum(shares))
summary(news$shares)

channel2 <- channel %>% subset(shares >= 2800) 

ggplot(data = channel2, aes(x=sentiment, y=shares)) +
  geom_col() +
  labs(x="sentiment", y="Number of shares", title = "Sentiment vs Shares")
```
This graph took the 25% of articles with the most shares and display the shares based on their positivity or negativity. The higher bar will indicate what the most shared articles can be categorized as that sentiment.

#Subjectivity
Is the title subjective?  Do shares lean towards opinion over objectiveness?  We would venture yes!

```{r}
plot(channel$title_subjectivity)

head(channel$title_subjectivity)

# Looks like the data is a ranking 0 to 1, and is continuous.  Scatterplot time! :)

g2 <- ggplot(channel, aes(x = title_subjectivity, y = shares))
g2 + geom_point() +
  labs(x="Title subjectivity", y  = "Number of shares", title = "Title subjectivity vs Shares")

```

   
```{r}
channel <- channel %>% mutate(subjectivity = if_else(title_subjectivity <=.3, "No Subjectivity", if_else(title_subjectivity <= .7, "Somewhat Subjective", "Very Subjective")))


#### ISSUE  ggplot(data=business2, aes(x=subjectivity, y=shares)) +   geom_col()

```
If a title had a subjectivity rating below 0.3 it was classified as Not subjective, between 0.3 and 0.7 as somewhat subjective, and over 0.7 very subjective. The highest bar in any column would indicate that the most shared articles have a subjectivity of that bars rating.  A high bar in the very subjective column would indicate that the title subjectivity was hard to classify and may contribute to a higher missclassification rate.





"After I pee my rsqured is much lower." (Kristi Ramey 10/22/2021 4:37 PM)

Just checking if you're actually reading this :) (we completely understand if you’re just skimming – we would.)

Moving on...

#Data analysis prep
Splitting data into a training and test set (70/30):
```{r message=FALSE, warning=FALSE}

# seed was already set...

# indices to split on
# want 70% to train on
trainIndex <- createDataPartition(channel$shares, p = .7, list = FALSE)


# subsets of data
channelTrain <- channel[trainIndex, ]
channelTest <- channel[-trainIndex, ]


```
#Linear Models

A linear model seeks to minimize the sum of the squared distances between the observed values and the predicted values. In this scenario the number of shares are the observed values. The predicted number of shares will be determined by the variables that are used in each model. Predicted values are found by inputting a value for each model variable and then applying the model coefficients to those inputs and summing the results. The coefficients associated with each model variable are the ones that minimized the squared distance between that prediction and the observed value. Adding more variables increases the complexity of the model but may also increase the predictive ability of the model. Using terms that are transformations of the original variables (such as squaring) can add more flexibility to the model allowing for increased predictive ability. We can evaluate a model by examining measures such as the mean squared error which averages the distances between the predictions and observed values (we want this number to be small), the R-squared value which tells us how much of the variation is explained by the model variables (we want this number to be near 1), or residual plots which illuminate potential outlier values or influential points.

```{R}
# Linear model using all of the variables in our EDA

# During our EDA, we noticed a handful of shares that were off the charts.  To bring down the range of our model's response, we are going to 'log' the shares variable.
log_shares <- log(channelTest$shares)

LM0 <- train(log(shares) ~ n_tokens_title + n_tokens_content + num_imgs +num_videos + global_sentiment_polarity +is_weekend, data = channelTrain,
             method = "lm", 
             preProcess = c('center', 'scale'),
             trControl = trainControl(method = 'cv', number = 10))
LM0

pred0 <- predict(LM0, newdata = channelTest)
r0 <- postResample(pred0, obs =log_shares)


# Linear model involving only multimedia

LM1 <- train(log(shares) ~ num_imgs + num_videos, data = channelTrain,
             method = "lm", 
             preProcess = c('center', 'scale'),
             trControl = trainControl(method = 'cv', number = 10))
LM1

pred1 <- predict(LM1, newdata = channelTest)
r1 <- postResample(pred1, obs = log_shares)


# Linear model based on when it was released Mon - Thur vs Fri - Sun

LM2 <- train(log(shares) ~ workweek, data = channelTrain,
             method = "lm", 
             preProcess = c('center', 'scale'),
             trControl = trainControl(method = 'cv', number = 10))
LM2

pred2 <- predict(LM2, newdata = channelTest)
r2 <- postResample(pred2, obs = log_shares)

# Linear model based on length

LM3 <- train(log(shares) ~ I(n_tokens_content)^2 + n_tokens_title, data = channelTrain,
             method = "lm", 
             preProcess = c('center', 'scale'),
             trControl = trainControl(method = 'cv', number = 10))
LM3

pred3 <- predict(LM3, newdata = channelTest)
r3 <- postResample(pred3, obs = log_shares)



```
Choose the best linear model

```{r}
linear_models <- c("Model 1", "Model 2", "Model 3", "Model 4")
linear_rmse <- c(r0[[1]],r1[[1]],r2[[1]],r3[[1]])
linear_r_squared <- c(LM0$results$Rsquared, LM1$results$Rsquared, LM2$results$Rsquared,LM3$results$Rsquared)
compare_linear_models <- data.frame(linear_models,linear_rmse, linear_r_squared)

compare_linear_models <- compare_linear_models %>% arrange(linear_r_squared)
best_linear_model <- compare_linear_models[4,]
best_linear_model
```
The best linear model is `r best_linear_model[[1]]` with an adjusted r squared value of `r best_linear_model[[3]]`.




#Random Forest Model
Random forest modelling uses the idea of bootstrapping. This method will take bootstrap samples form the data, train the tree using that sample, repeat a set number of times and return a final prediction that is the mean of the predictions found in this process. Random forest is different from bagging in that it uses a random subset of the predictors for each bootstrap. This is potentially more useful if there are correlated predictors. Correlated predictors lead to trees that will be correlated and random forest allows for more independence so that variation will be reduced. 

We will use parallel computing to speed this process up!
```{r message=FALSE, warning=FALSE}

# Assign the task

cl<-makePSOCKcluster(12)
  
registerDoParallel(cl)

random_forest <- train(log(shares) ~ n_tokens_title + n_tokens_content + num_imgs +num_videos + global_sentiment_polarity +is_weekend , data = channelTrain,
          method = "rf", 
          trControl = trainControl(method = "cv", number = 5),
          preProcess = c("center", "scale"),
          tuneGrid = data.frame(mtry = 1:5))

stopCluster(cl)
random_forest

predrf <- predict(random_forest, newdata = channelTest)
r4 <- postResample(predrf, obs = log_shares)

unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}
unregister_dopar()
```

#Boosted Tree Method:
The Boosted Tree Method is like the Random Forest Method, in that it continues to break the model into family trees based on the most impactful predictors, however the mechanisms to do so differ.
The BTM uses iterations with each ‘generation’ and will recalibrate the model at each level.  This results in the new tree being updated by the previous tree.  We begin our Boost with an initialized prediction of zero and create a tree model using the residuals as the response.  It will then reconstruct the model, for a new fit and repeat this process for the number of times we selected.  
The BTM typically is the best method, since it is a more deliberate and a slow fitting process that updates as it goes.

```{r message=FALSE, warning=FALSE}

# Boosted Tree Model:


# We wanted to continue doing the log of our response, however, since we were using the 'Poisson' distribution, our values had to be positive.  In order to achieve that, we translated our response to the left 1.
boostFit <- gbm(log(shares) ~ n_tokens_title + n_tokens_content + 
                      num_imgs +num_videos + global_sentiment_polarity + 
                      is_weekend , data = channelTrain,
          distribution = "gaussian", 
          n.trees = 5000,
          shrinkage = .1, interaction.depth = 4)

boostPred <- predict(boostFit, newdata = channelTest)
boostedRMSE <- sqrt(mean((boostFit$fit - log_shares)^2))
boostedRMSE


```

#Compare Models
```{r}

model_types <- c("Linear Model", "Random Forest","Boosted Tree")
model_rmse <- c(best_linear_model[[2]], r4[[1]], boostedRMSE[[1]])
compare_models <- data.frame(model_types, model_rmse)

compare_models <- compare_models %>% arrange((model_rmse))
best_model <- compare_models[1,1]
best_model_rmse <- compare_models[1,2]

```
The best model for `r page_name` shares is `r best_model` since it has the lowest RMSE at `r best_model_rmse`.
